{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe31d63-600e-40f5-b0a9-475567e5787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from transformers import Trainer, TrainingArguments, BartTokenizer, BartForSequenceClassification\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import IPython.display\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59f034e-1eaf-4a4a-8973-5ca5f169f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, file_path):\n",
    "        # Tokenization and cleaning related variable\n",
    "        self.regex_tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+|\\.(?![a-zA-Z0-9])\")\n",
    "        \n",
    "        # Model related variables\n",
    "        self.max_token = 128\n",
    "        self.id2label = {0: \"rulings\", 1: \"facts\", 2: \"issues\"}\n",
    "        self.label2id = {\"rulings\": 0, \"facts\": 1, \"issues\": 2}\n",
    "        self.BART_tokenizer  = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.BART_model  = BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", num_labels=3, \n",
    "                                                                      id2label=self.id2label, label2id=self.label2id,\n",
    "                                                                      problem_type=\"single_label_classification\")\n",
    "\n",
    "        # Data related variables\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.two_sentence = []\n",
    "        self.two_sentence_tokens = []\n",
    "        self.tokenized_sentences = []\n",
    "        self.segment_labels = []\n",
    "        self.data = {}\n",
    "\n",
    "        # Final preprocessing output\n",
    "        self.train_dataset = None\n",
    "        self.eval_dataset = None\n",
    "        \n",
    "        # Unknown token variables\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Preprocess and prepare raw data\n",
    "        self.df.dropna(inplace=True)\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        self.preprocess()\n",
    "\n",
    "        # Find and store unknown tokens\n",
    "        self.find_unknown_token()\n",
    "        self.set_model_configuration()\n",
    "\n",
    "    def return_model_tokenizer_data(self):\n",
    "        print('Model configuration: ', self.BART_model.config)\n",
    "        return self.BART_model, self.BART_tokenizer, self.train_dataset, self.eval_dataset\n",
    "\n",
    "    def prepare_LED_data(self):\n",
    "        self.data = {\n",
    "            'text': self.tokenized_sentences,\n",
    "            'labels': self.segment_labels,\n",
    "        }\n",
    "\n",
    "        self.df = pd.DataFrame.from_dict(self.data)\n",
    "\n",
    "        train_data, eval_data = train_test_split(self.df, test_size=0.1, random_state=42)\n",
    "\n",
    "        train_data = Dataset.from_pandas(train_data)\n",
    "        eval_data = Dataset.from_pandas(eval_data)\n",
    "\n",
    "        # Map \n",
    "        self.train_dataset = train_data.map(\n",
    "            self.process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\", '__index_level_0__']\n",
    "        )\n",
    "\n",
    "        self.eval_dataset = eval_data.map(\n",
    "            self.process_data_to_model_inputs,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\", '__index_level_0__']\n",
    "        )\n",
    "\n",
    "        self.train_dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        )\n",
    "        self.eval_dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        )\n",
    "\n",
    "    def process_data_to_model_inputs(self, batch):\n",
    "        # Tokenize the inputs\n",
    "        inputs = self.BART_tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "        # Prepare input IDs and attention masks\n",
    "        batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "        batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def preprocess(self):\n",
    "        '''\n",
    "        Clean characters and tokenize court cases and segments.\n",
    "        '''\n",
    "        # Lowercase the text and Remove unnecessary characters\n",
    "        self.two_sentence = [self.change_char(text.lower()) for text in self.df[\"heading\"]]\n",
    "\n",
    "        # Tokenize the text, storing words and numbers only\n",
    "        self.two_sentence_tokens = [self.regex_tokenizer.tokenize(text) for text in self.two_sentence]\n",
    "        \n",
    "        # Join tokens to form full strings for each case\n",
    "        self.two_sentence = [' '.join(token) for token in self.two_sentence_tokens]\n",
    "\n",
    "        # Create label-to-ID mapping\n",
    "        label_mapping = {\"facts\": 0, \"issues\": 1, \"ruling\": 2}\n",
    "        self.segment_labels = [label_mapping[label] for label in self.df[\"label\"]]\n",
    "\n",
    "        # Tokenize each string into sentences\n",
    "        self.tokenized_sentences = [sent_tokenize(sentence)[:5] for sentence in self.two_sentence]\n",
    "\n",
    "        # Add label to each tokenized sentence\n",
    "        self.segment_labels = [[label] * len(tokens) for label, tokens in zip(self.segment_labels, self.tokenized_sentences)]\n",
    "\n",
    "        # Flatten the lists while ensuring consistent length between tokenized sentences and segment labels\n",
    "        filtered_data = [(sentence, label) for sublist, label_sublist in zip(self.tokenized_sentences, self.segment_labels)\n",
    "                         for sentence, label in zip(sublist, label_sublist) if len(self.regex_tokenizer.tokenize(sentence)) <= self.max_token]\n",
    "        \n",
    "        # Unzip the filtered data back into separate lists\n",
    "        self.tokenized_sentences, self.segment_labels = zip(*filtered_data) if filtered_data else ([], [])\n",
    "\n",
    "        # Convert to lists (if needed)\n",
    "        self.tokenized_sentences = list(self.tokenized_sentences)\n",
    "        self.segment_labels = list(self.segment_labels)\n",
    "\n",
    "        # Balance the labels by upsampling the minority classes\n",
    "        self.balance_labels()\n",
    "\n",
    "    def balance_labels(self):\n",
    "        '''\n",
    "        Downsample the majority classes to balance the dataset.\n",
    "        '''\n",
    "        # Create a DataFrame from tokenized sentences and labels for easy manipulation\n",
    "        df_balancing = pd.DataFrame({\n",
    "            'sentence': self.tokenized_sentences,\n",
    "            'label': self.segment_labels\n",
    "        })\n",
    "    \n",
    "        # Get the count of each class\n",
    "        label_counts = df_balancing['label'].value_counts()\n",
    "        min_count = label_counts.min()  # Find the size of the smallest class\n",
    "    \n",
    "        # Separate the DataFrame by label\n",
    "        df_facts = df_balancing[df_balancing['label'] == 0]\n",
    "        df_issues = df_balancing[df_balancing['label'] == 1]\n",
    "        df_rulings = df_balancing[df_balancing['label'] == 2]\n",
    "    \n",
    "        # Downsample the majority classes to match the smallest class\n",
    "        df_facts_downsampled = resample(df_facts, replace=False, n_samples=min_count, random_state=42)\n",
    "        df_issues_downsampled = resample(df_issues, replace=False, n_samples=min_count, random_state=42)\n",
    "        df_rulings_downsampled = resample(df_rulings, replace=False, n_samples=min_count, random_state=42)\n",
    "    \n",
    "        # Combine the downsampled dataframes\n",
    "        df_balanced = pd.concat([df_facts_downsampled, df_issues_downsampled, df_rulings_downsampled])\n",
    "    \n",
    "        # Shuffle the dataset to ensure randomness\n",
    "        df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "        # Update the tokenized sentences and segment labels with the balanced data\n",
    "        self.tokenized_sentences = df_balanced['sentence'].tolist()\n",
    "        self.segment_labels = df_balanced['label'].tolist()\n",
    "\n",
    "        # Clear the output and print value count\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        value_counts = df_balanced['label'].value_counts()\n",
    "        print(value_counts)\n",
    "        \n",
    "\n",
    "    def find_unknown_token(self):\n",
    "        '''\n",
    "        Iterate over each case, converting the tokens into IDs. Check if there is an unknown token in input_ids.\n",
    "        '''\n",
    "        unk_id = self.BART_tokenizer.unk_token_id\n",
    "        for tokens in self.two_sentence_tokens:\n",
    "            input_ids = self.BART_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            for i, token_id in enumerate(input_ids):\n",
    "                if token_id == unk_id and tokens[i] not in self.unknown_tokens:  # Check if the token is unknown\n",
    "                    self.found_new_unknown_token = True\n",
    "                    self.unknown_tokens.append(tokens[i])  # Add the unknown token to the list\n",
    "        print(f\"Added {len(self.unknown_tokens)} new tokens\\n\")\n",
    "\n",
    "    def set_model_configuration(self):\n",
    "        '''\n",
    "        Configure Model.\n",
    "        '''\n",
    "        # Add the new tokens to the tokenizer\n",
    "        if self.unknown_tokens:\n",
    "            self.BART_tokenizer.add_tokens(self.unknown_tokens)\n",
    "\n",
    "        # Resize the model's token embeddings to match the new tokenizer length\n",
    "        self.BART_model.resize_token_embeddings(len(self.BART_tokenizer))\n",
    "        \n",
    "        self.BART_model.config.max_position_embeddings = self.max_token\n",
    "        # Safely delete generation-related fields\n",
    "        if hasattr(self.BART_model.config, 'early_stopping'):\n",
    "            del self.BART_model.config.early_stopping\n",
    "        if hasattr(self.BART_model.config, 'num_beams'):\n",
    "            del self.BART_model.config.num_beams\n",
    "        if hasattr(self.BART_model.config, 'no_repeat_ngram_size'):\n",
    "            del self.BART_model.config.no_repeat_ngram_size\n",
    "        if hasattr(self.BART_model.config, 'forced_bos_token_id'):\n",
    "            del self.BART_model.config.forced_bos_token_id\n",
    "\n",
    "    def change_char(self, text):\n",
    "        \"\"\"\n",
    "        Cleans up text by removing or replacing certain characters and patterns.\n",
    "        \"\"\"\n",
    "        # Custom cleaning logic\n",
    "        text = re.sub(r\"section (\\d+)\\.\", r\"section \\1\", text)\n",
    "        text = re.sub(r\"sec.\", r\"sec\", text)\n",
    "        text = re.sub(r\"p.d.\", r\"pd\", text)\n",
    "        text = re.sub(r\"\\bno.\\b\", r\"number\", text)\n",
    "        text = re.sub(r\"\\brtc\\b\", \"regional trial court\", text)\n",
    "        text = re.sub(r\"[(),'\\\"’”\\[\\]]\", \" \", text)\n",
    "        text = re.sub(r\"[“”]\", \" \", text)\n",
    "        text = re.sub(r\"\\u2033\", \" \", text)\n",
    "        text = re.sub(r\"\\u2032\", \" \", text)\n",
    "        text = re.sub(r\"\\bg\\b\", \" \", text)\n",
    "        text = re.sub(r\"\\br\\b\", \" \", text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fbae39a-859c-4f5a-906d-1ceaad4989b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    5969\n",
      "1    5969\n",
      "0    5969\n",
      "Name: count, dtype: int64\n",
      "tae\n",
      "Added 36773 new tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data, model, and tokenizer before training\n",
    "preprocessor = preprocess('court_cases_headings_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a074a21-0e8d-4b48-9513-1cc6ddabc504",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(preprocessor.BART_tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6b847-4175-4709-a433-6622e334e0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bf69d4-de5d-4d6a-a314-baecd0ef72e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04f651ecf8542dc9917434c176acf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855ee8ed362a41a3bcc9d6b8f3a50229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1791 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor.prepare_LED_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5914ce92-a2a4-4eed-99d5-6cad8653da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_model, BART_tokenizer, train_data, eval_data = preprocessor.return_model_tokenizer_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f0ad21-3f33-4e0d-bf33-a58cb36a0bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([    0,  4321,  2137,  1200,   147, 40654,  1571,    13,   872,     9,\n",
       "          7074,  2148,    21,  4241,   963,    10, 17082,  7068,   281,   368,\n",
       "            50,    10,  1837,  1060,  1760,    21,     5, 43860, 16633,  1303,\n",
       "             9,     5,   744,     9,     5,  1802,   223,  1566,   132, 29431,\n",
       "             9,     5,  2366,  3260,   479,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3f219b-2c47-4bda-9b74-3177d2a00d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16116])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c4a3a1-3d2b-427b-8955-f77d47bc8c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16116, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ef2340d-4e6f-4698-8e5d-212b977b5017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16116, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66987097-9125-46b4-866d-963becdf6e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
