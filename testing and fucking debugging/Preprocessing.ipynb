{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "971cd455-2154-4122-8753-be7924814ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c884ae0-ead1-4616-b171-828f37e64824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, file_path, bert_tokenizer):\n",
    "        self.df = pd.read_csv('new_court_cases.csv')\n",
    "        self.tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.window_size = 128\n",
    "        self.max_token = 510\n",
    "        self.court_cases = None\n",
    "        self.rulings = None\n",
    "        self.issues = None\n",
    "        self.facts = None\n",
    "\n",
    "        # Flag for new tokens found\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "\n",
    "        # use this variable for debugging\n",
    "        self.debugging = True\n",
    "\n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess()\n",
    "\n",
    "        # drop duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "    def preprocess(self):\n",
    "        # lowercase the text and Remove unnecessary characters\n",
    "        self.court_cases = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.rulings = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.facts = [self.change_char(text.lower())for text in self.df[\"facts\"]]\n",
    "        self.issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "\n",
    "        # tokenize the text, storing words only\n",
    "        self.court_cases = [self.tokenizer.tokenize(text) for text in self.court_cases]\n",
    "        self.rulings = [self.tokenizer.tokenize(text) for text in self.rulings]\n",
    "        self.facts = [self.tokenizer.tokenize(text) for text in self.facts]\n",
    "        self.issues = [self.tokenizer.tokenize(text) for text in self.issues]\n",
    "\n",
    "        # if longer than 512 tokens, chunk the tokens into 512 while adding windows\n",
    "        self.court_cases = [self.split_to_chunks_with_windows(tokens) for tokens in self.court_cases]\n",
    "        self.rulings = [self.split_to_chunks_with_windows(tokens) for tokens  in self.rulings]\n",
    "        self.facts = [self.split_to_chunks_with_windows(tokens) for tokens  in self.facts]\n",
    "        self.issues = [self.split_to_chunks_with_windows(tokens) for tokens  in self.issues]\n",
    "\n",
    "\n",
    "    def split_to_chunks_with_windows(self, tokens):\n",
    "        \"\"\"\n",
    "        Court cases has long text while BERT only has 512 maximum tokens. \n",
    "        This function splits the long text into chunks with 512 tokens with sliding windows.\n",
    "\n",
    "            Args:\n",
    "                tokens: A list of tokens.\n",
    "\n",
    "            Returns:\n",
    "                chunks: A list of list of string with 512 tokens including BERT's special token ([CLS] [SEP])\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        # Iterate over the list of tokens and chunking them with window slides\n",
    "        for i in range(0, len(tokens), self.max_token - self.window_size):\n",
    "            # store the current tokens \n",
    "            chunk = tokens[i:i+self.max_token]\n",
    "\n",
    "            # add special tokens\n",
    "            chunk.insert(0, \"[CLS]\")\n",
    "            chunk.append(\"[SEP]\")\n",
    "            \n",
    "            # convert to string format\n",
    "            chunk_string = ' '.join(chunk)\n",
    "\n",
    "            # append\n",
    "            chunks.append(chunk_string)\n",
    "\n",
    "            # Break loop if we have covered the entire sequence\n",
    "            if i + self.max_token >= len(tokens):\n",
    "                break\n",
    "                \n",
    "        return chunks\n",
    "\n",
    "    def change_char(self, text):\n",
    "        \"\"\"\n",
    "        Removes special characters from a list of strings using regular expressions.\n",
    "        As well as change characters.\n",
    "        \n",
    "        \n",
    "          Args:\n",
    "            strings: A list of strings.\n",
    "        \n",
    "          Returns:\n",
    "            A new list of strings without special characters.\n",
    "          \"\"\"\n",
    "        text = re.sub(r'[(),:;\\'\".’”[]]', '', text)\n",
    "        text = re.sub(r'rtc', 'regional trial court', text)\n",
    "        text = re.sub(r\"\\w*\\d+\\w*\", \"\", text)\n",
    "        text = re.sub(r\"“\", \"\", text)\n",
    "        text = re.sub(r\",”\", \"\", text)\n",
    "        text = re.sub(r\",\", \"\", text)\n",
    "        text = re.sub(r\",,.\", \"\", text)\n",
    "        text = re.sub(r\",,.,\", \"\", text)\n",
    "        text = re.sub(r\"--,\", \"\", text)\n",
    "        '''text = re.sub(r\"--\", \"\", text)\n",
    "        text = re.sub(r\".\", \"\", text)\n",
    "        text = re.sub(r\"—\", \"\", text)\n",
    "        text = re.sub(r\"],\", \"\", text)'''\n",
    "        text = re.sub(r\"\\u2033\", \"\", text)\n",
    "        text = re.sub(r\"\\u2032\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def prepare_input_output(self, chunks):\n",
    "        \"\"\"\n",
    "        Prepare input-output pairs for each chunk. \n",
    "        Returns a list of tuples, where each tuple represents an (input, output) pair.\n",
    "        \"\"\"\n",
    "        input_output_pairs = []\n",
    "        for chunk in chunks:\n",
    "            # Tokenize the chunk and convert to IDs\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(chunk.split())\n",
    "    \n",
    "            # Verify that the chunk ends with the [SEP] token to avoid duplicates\n",
    "            if input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"):\n",
    "                input_ids.append(self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "    \n",
    "            # Prepare the shifted output (excluding the initial [CLS] token)\n",
    "            shifted_output = input_ids[1:]  # Shifted output starts from the second token\n",
    "    \n",
    "            # No need to append [SEP] here, it's already included in input_ids if required\n",
    "            # Add input-output pair to list\n",
    "            input_output_pairs.append((input_ids, shifted_output))\n",
    "\n",
    "            # Check for unknown tokens and append them to the list that will be added\n",
    "            unk_tokens = chunk.split()\n",
    "            for i in range(len(unk_tokens)):\n",
    "                if input_ids[i] == 100 and unk_tokens[i] not in self.unknown_tokens:\n",
    "                    print(unk_tokens[i],\" : \",input_ids[i])\n",
    "                    self.found_new_unknown_token = True\n",
    "                    self.unknown_tokens.append(unk_tokens[i])\n",
    "                        \n",
    "            \n",
    "        return input_output_pairs\n",
    "\n",
    "    def get_training_data(self):\n",
    "        \"\"\"\n",
    "        Prepare training data for all segments, maintaining the structure per court case.\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        self.found_new_unknown_token = False\n",
    "        \n",
    "        for i in range(len(self.court_cases)):\n",
    "            # Prepare input-output pairs for each segment within a single court case\n",
    "            court_case_data = self.court_cases[i]\n",
    "            ruling_data = self.prepare_input_output(self.rulings[i])\n",
    "            fact_data = self.prepare_input_output(self.facts[i])\n",
    "            issue_data = self.prepare_input_output(self.issues[i])\n",
    "            \n",
    "            # Maintain structure by grouping segments within the same court case\n",
    "            case_data = {\n",
    "                \"court_case\": court_case_data,\n",
    "                \"rulings\": ruling_data,\n",
    "                \"facts\": fact_data,\n",
    "                \"issues\": issue_data\n",
    "            }\n",
    "            \n",
    "            training_data.append(case_data)\n",
    "\n",
    "        # If unknown token/s found, Update file containing all unknown token & Raise an error message\n",
    "        if self.unknown_tokens and self.found_new_unknown_token:\n",
    "            with open('unknown_tokens.txt', 'w') as f:\n",
    "                for token in self.unknown_tokens:\n",
    "                    f.write(f\"{token}\\n\")\n",
    "            raise Exception(\"There are unknown token/s found. Update the tokenizer and finetune the model.\")\n",
    "        \n",
    "        return training_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcaa0edb-c143-4ea2-a9eb-473220c57ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prep_model:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.update_tokenizer()\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            raise Exception(\"No file found.\")\n",
    "\n",
    "    def update_tokenizer(self):\n",
    "        \n",
    "        # Add the new tokens to the tokenizer\n",
    "        self.tokenizer.add_tokens(self.unknown_tokens)\n",
    "\n",
    "        # Resize the model's token embeddings to match the new tokenizer length\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def finetune_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b872989e-724d-4318-a2ce-d3f925d0b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TopicSegmentation import LegalBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d6d755c-49ab-4af5-ad2f-a641b73f8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor and legal BERT\n",
    "legal_bert = LegalBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5be7d24-f5a9-40e5-ab18-5033d6251cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = preprocess(\"new_court_cases.csv\", legal_bert.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86e56fb6-9949-495b-8895-8cced4b4c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = x.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaec7ec-842d-4902-a625-8c1ee2a40dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
