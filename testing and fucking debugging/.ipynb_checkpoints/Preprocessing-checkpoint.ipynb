{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cd455-2154-4122-8753-be7924814ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcdfb4-8eb1-4e95-bf15-2e843d8243e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+|\\.(?![a-zA-Z0-9])\")\n",
    "        self.window_size = 128\n",
    "        self.max_token = 510\n",
    "        \n",
    "    def split_to_chunks_with_windows_with_attention_mask(self, tokens, labels=None):\n",
    "        \"\"\"\n",
    "        Court cases has long text while BERT only has 512 maximum tokens. \n",
    "        This function splits the long text into chunks with 512 tokens with sliding windows.\n",
    "        Add necessary padding if not equal to 512 tokens.\n",
    "        Add attention mask.\n",
    "\n",
    "            Args:\n",
    "                tokens: A list of tokens.\n",
    "\n",
    "            Returns:\n",
    "                chunks: A list of list of string with 512 tokens including BERT's special token ([CLS] [SEP])\n",
    "                chunk_labels: A list of lists, each containing labels corresponding to the tokens in chunks.\n",
    "                attention_masks: A list of list of 1s and 0s that portrays what places are masked (0)\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_labels = []\n",
    "        attention_masks = []\n",
    "\n",
    "        # Iterate over the list of tokens and chunking them with window slides\n",
    "        for i in range(0, len(tokens), self.max_token - self.window_size):\n",
    "            # store the current window of tokens and labels\n",
    "            chunk_tokens = tokens[i:i + self.max_token]\n",
    "            if labels:\n",
    "                chunk_label = labels[i:i + self.max_token]\n",
    "\n",
    "            # add special tokens\n",
    "            chunk_tokens.insert(0, \"[CLS]\")\n",
    "            chunk_tokens.append(\"[SEP]\")\n",
    "\n",
    "            if labels:\n",
    "                # Add corresponding labels for special tokens\n",
    "                chunk_label.insert(0, -100)  # Label for [CLS]\n",
    "                chunk_label.append(-100)     # Label for [SEP]\n",
    "\n",
    "            # Add padding and mask if less than 512 tokens\n",
    "            if len(chunk_tokens) < self.max_token + 2: # +2 for [CLS] and [SEP]\n",
    "                chunk_tokens, mask = self.pad_and_mask(chunk_tokens, maxlength=self.max_token + 2)\n",
    "                if labels:\n",
    "                    chunk_label.extend([-100] * (self.max_token + 2 - len(chunk_label)))  # Padding labels with -100 as well\n",
    "            else: # just add the mask\n",
    "                mask = np.ones(512, dtype=int).tolist()\n",
    "\n",
    "            # append\n",
    "            chunks.append(chunk_tokens)\n",
    "            attention_masks.append(mask)\n",
    "            if labels:\n",
    "                chunk_labels.append(chunk_label)\n",
    "\n",
    "            # Break loop if we have covered the entire sequence\n",
    "            if i + self.max_token >= len(tokens):\n",
    "                break\n",
    "\n",
    "        # If used for finetuning\n",
    "        if labels:\n",
    "            return chunks, chunk_labels, attention_masks\n",
    "        else:\n",
    "            return (chunks, attention_masks)\n",
    "\n",
    "    def pad_and_mask(self, chunk, maxlength=512):\n",
    "        \"\"\"\n",
    "        Add [PAD] tokens to the chunk to make its length equal to maxlength (512 tokens).\n",
    "        Add mask attention.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The list of tokens.\n",
    "            maxlength: The target length after padding (default is 512).\n",
    "        \n",
    "        Returns:\n",
    "            chunk: list of tokens with paddings.\n",
    "            attention_mask: list of 1s and 0s for masking the paddings.\n",
    "        \"\"\"\n",
    "        attention_mask = []\n",
    "        \n",
    "        # Calculate how many [PAD] tokens are needed\n",
    "        pads_to_add = maxlength - len(chunk)\n",
    "\n",
    "        # Add the attention mask\n",
    "        attention_mask = [1] * len(chunk) + [0] * pads_to_add\n",
    "    \n",
    "        # Extend the chunk with [PAD] tokens\n",
    "        chunk.extend([\"[PAD]\"] * pads_to_add)\n",
    "\n",
    "        return chunk, attention_mask\n",
    "\n",
    "    def change_char(self, text):\n",
    "        \"\"\"\n",
    "        Removes special characters from a list of strings using regular expressions.\n",
    "        As well as change characters.\n",
    "        \n",
    "        \n",
    "          Args:\n",
    "            strings: A list of strings.\n",
    "        \n",
    "          Returns:\n",
    "            A new list of strings without special characters.\n",
    "          \"\"\"\n",
    "        text = re.sub(r'[(),:;\\'\"’”[]]', '', text)\n",
    "        text = re.sub(r'rtc', 'regional trial court', text)\n",
    "        text = re.sub(r\"\\w*\\d+\\w*\", \"\", text)\n",
    "        text = re.sub(r\"“\", \"\", text)\n",
    "        text = re.sub(r\",”\", \"\", text)\n",
    "        text = re.sub(r\",\", \"\", text)\n",
    "        text = re.sub(r\",,.\", \"\", text)\n",
    "        text = re.sub(r\",,.,\", \"\", text)\n",
    "        text = re.sub(r\"--,\", \"\", text)\n",
    "        text = re.sub(r\"\\bno.\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\bg\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\br\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\u2033\", \"\", text)\n",
    "        text = re.sub(r\"\\u2032\", \"\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c884ae0-ead1-4616-b171-828f37e64824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_seqtoseq_data(preprocess):\n",
    "    def __init__(self, file_path, bert_tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.court_cases = None\n",
    "        self.rulings = None\n",
    "        self.issues = None\n",
    "        self.facts = None\n",
    "\n",
    "        # Flag for new tokens found\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "\n",
    "        # use this variable for debugging\n",
    "        self.debugging = True\n",
    "\n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess()\n",
    "\n",
    "        # drop duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "    def preprocess(self):\n",
    "        # lowercase the text and Remove unnecessary characters\n",
    "        self.court_cases = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.rulings = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.facts = [self.change_char(text.lower())for text in self.df[\"facts\"]]\n",
    "        self.issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "\n",
    "        # tokenize the text, storing words only\n",
    "        self.court_cases = [self.tokenizer.tokenize(text) for text in self.court_cases]\n",
    "        self.rulings = [self.tokenizer.tokenize(text) for text in self.rulings]\n",
    "        self.facts = [self.tokenizer.tokenize(text) for text in self.facts]\n",
    "        self.issues = [self.tokenizer.tokenize(text) for text in self.issues]\n",
    "        \n",
    "        # if longer than 512 tokens, chunk the tokens into 512 while adding windows and paddings & attention mask\n",
    "        self.court_cases = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.court_cases]\n",
    "        self.rulings = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.rulings]\n",
    "        self.facts = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.facts]\n",
    "        self.issues = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.issues]\n",
    "\n",
    "    def prepare_input_output(self, chunks):\n",
    "        \"\"\"\n",
    "        Prepare input-output pairs for each chunk. \n",
    "        Returns a list of tuples, where each tuple represents an (input, output) pair.\n",
    "        \"\"\"\n",
    "        input_output_pairs = []\n",
    "        for chunk in chunks:\n",
    "            # Tokenize the chunk and convert to IDs\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(chunk)\n",
    "    \n",
    "            # Verify that the chunk ends with the [SEP] token to avoid duplicates\n",
    "            not_sep = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "            not_pad = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "            if not_sep and not_pad:\n",
    "                input_ids.append(self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "    \n",
    "            # Prepare the shifted output (excluding the initial [CLS] token)\n",
    "            shifted_output = input_ids[1:]  # Shifted output starts from the second token\n",
    "    \n",
    "            # No need to append [SEP] here, it's already included in input_ids if required\n",
    "            # Add input-output pair to list\n",
    "            input_output_pairs.append((input_ids, shifted_output))\n",
    "\n",
    "            # Check for unknown tokens and append them to the list that will be added\n",
    "            unk_tokens = chunk\n",
    "            for i in range(len(unk_tokens)):\n",
    "                if input_ids[i] == 100 and unk_tokens[i] not in self.unknown_tokens:\n",
    "                    print(unk_tokens[i],\" : \",input_ids[i])\n",
    "                    self.found_new_unknown_token = True\n",
    "                    self.unknown_tokens.append(unk_tokens[i])\n",
    "\n",
    "            '''if self.debugging == True:\n",
    "                self.debugging = False\n",
    "                print(\"output of the decoder: \",shifted_output)\n",
    "                print(\"input to the decoder:\",input_ids)'''\n",
    "                        \n",
    "            \n",
    "        return input_output_pairs\n",
    "\n",
    "    def prepare_court_case(self, chunks):\n",
    "        \"\"\"\n",
    "        Convert court cases tokens into their respective IDs.\n",
    "\n",
    "            Args:\n",
    "                chunks: A list of tokens.\n",
    "        \n",
    "            Returns:\n",
    "                \"court_cases_ids\", a list of of list of IDs (integers).\n",
    "        \"\"\"\n",
    "        court_cases_ids = []\n",
    "        for chunk in chunks:\n",
    "            # Tokenize the chunk and convert to IDs\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(chunk)\n",
    "    \n",
    "            # Verify that the chunk ends with the [SEP] token to avoid duplicates\n",
    "            not_sep = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "            not_pad = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "            if not_sep and not_pad:\n",
    "                input_ids.append(self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "\n",
    "            court_cases_ids.append(input_ids)\n",
    "                \n",
    "        return court_cases_ids\n",
    "\n",
    "    def get_training_data(self):\n",
    "        \"\"\"\n",
    "        Prepare training data for all segments, maintaining the structure per court case.\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        self.found_new_unknown_token = False\n",
    "        \n",
    "        for i in range(len(self.court_cases)):\n",
    "            # Prepare input-output pairs for each segment within a single court case\n",
    "            court_case_data = self.prepare_court_case(self.court_cases[i][0])\n",
    "            ruling_data = self.prepare_input_output(self.rulings[i][0])\n",
    "            fact_data = self.prepare_input_output(self.facts[i][0])\n",
    "            issue_data = self.prepare_input_output(self.issues[i][0])\n",
    "            \n",
    "            # Maintain structure by grouping segments within the same court case\n",
    "            case_data = {\n",
    "                \"court_case\": court_case_data,\n",
    "                \"rulings\": ruling_data,\n",
    "                \"facts\": fact_data,\n",
    "                \"issues\": issue_data\n",
    "            }\n",
    "\n",
    "            '''print(type(ruling_data))\n",
    "            print(type(ruling_data[0]))\n",
    "            print(type(ruling_data[0][0]))'''\n",
    "            \n",
    "            training_data.append(case_data)\n",
    "\n",
    "        # If unknown token/s found, Update file containing all unknown token & Raise an error message\n",
    "        if self.unknown_tokens and self.found_new_unknown_token:\n",
    "            with open('unknown_tokens.txt', 'w') as f:\n",
    "                for token in self.unknown_tokens:\n",
    "                    f.write(f\"{token}\\n\")\n",
    "            raise Exception(\"There are unknown token/s found. Update the tokenizer and finetune the model.\")\n",
    "        \n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631f11d-f06c-48c4-a6c5-634bb3eb2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_finetuning_data(preprocess):\n",
    "    def __init__(self, file_path, bert_tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "        # Flag for new tokens found\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "    \n",
    "        # For fine-tuning\n",
    "        self.finetune_court = None\n",
    "        self.finetune_ruling = None\n",
    "        self.finetune_issues = None\n",
    "        self.finetune_facts = None\n",
    "        self.finetune_data = []\n",
    "\n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # preprocess and prepare proper format of data for finetuning\n",
    "        self.preprocess()\n",
    "        self.prepare_finetune_data()\n",
    "\n",
    "        # drop duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "         \n",
    "    def preprocess(self):\n",
    "        # lowercase the text and Remove unnecessary characters\n",
    "        self.finetune_court = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.finetune_ruling = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.finetune_facts = [self.change_char(text.lower())for text in self.df[\"facts\"]]\n",
    "        self.finetune_issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "\n",
    "        # tokenize the text, accepting words, numbers, and dots only\n",
    "        self.finetune_court = [self.tokenizer.tokenize(text) for text in self.finetune_court]\n",
    "        self.finetune_ruling = [self.tokenizer.tokenize(text) for text in self.finetune_ruling]\n",
    "        self.finetune_facts = [self.tokenizer.tokenize(text) for text in self.finetune_facts]\n",
    "        self.finetune_issues = [self.tokenizer.tokenize(text) for text in self.finetune_issues]\n",
    "\n",
    "        # convert to whole string\n",
    "        self.finetune_court = [' '.join(token) for token in self.finetune_court]\n",
    "        self.finetune_ruling = [' '.join(token) for token in self.finetune_ruling]\n",
    "        self.finetune_facts = [' '.join(token) for token in self.finetune_facts]\n",
    "        self.finetune_issues = [' '.join(token) for token in self.finetune_issues]\n",
    "\n",
    "        # split into tokens (sentences)\n",
    "        self.finetune_court = [sent_tokenize(text) for text in self.finetune_court]\n",
    "        self.finetune_ruling = [sent_tokenize(text) for text in self.finetune_ruling]\n",
    "        self.finetune_facts = [sent_tokenize(text) for text in self.finetune_facts]\n",
    "        self.finetune_issues = [sent_tokenize(text) for text in self.finetune_issues]\n",
    "            \n",
    "    def prepare_finetune_data(self):\n",
    "        \"\"\"\n",
    "        Prepare token classification data, labeling each token in the court case with\n",
    "        its corresponding segment (rulings, facts, or issues).\n",
    "\n",
    "        Variables:\n",
    "            tokens_and_labels: list of tuples, wherein each tuples contains the list of 512 tokens and the list of labels of those 512 tokens.\n",
    "            case_labels: list of integers, wherein each integer corresponds to a court case segment (i.e. rulings = 0).\n",
    "            new_tokens: list of tokens, wherein each token is a sentence of a corresponding segment label.\n",
    "            chunks:\n",
    "            chunk_labels:\n",
    "            attention_masks:\n",
    "        \"\"\"\n",
    "        for i in range(len(self.finetune_court)):\n",
    "            # Create context-based labels for the entire court case\n",
    "            case_labels, new_tokens = self.prepare_contextual_labels(self.finetune_court[i], self.finetune_ruling[i], \n",
    "                                                         self.finetune_facts[i], self.finetune_issues[i])\n",
    "\n",
    "            chunks, chunk_labels, attention_masks = self.split_to_chunks_with_windows_with_attention_mask(new_tokens, case_labels)\n",
    "\n",
    "            for i in range(len(chunks)):\n",
    "                self.finetune_data.append({\n",
    "                    \"input_ids\": self.bert_tokenizer.convert_tokens_to_ids(chunks[i]),\n",
    "                    \"labels\": chunk_labels[i],\n",
    "                    \"attention_mask\": attention_masks[i]\n",
    "                })\n",
    "\n",
    "    def prepare_contextual_labels(self, case_tokens, ruling_tokens, fact_tokens, issue_tokens):\n",
    "        \"\"\"\n",
    "        Given the tokenized court case, assign labels to each token based on context.\n",
    "\n",
    "            Returns:\n",
    "                labels: list of integers corresponding to their court casesegment label\n",
    "                new_tokens: list of tokens, wherein each tokens is a sentence converted to word format.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        new_tokens = []\n",
    "        \n",
    "        for text in case_tokens:\n",
    "            token = self.tokenizer.tokenize(text)\n",
    "            if text in fact_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([1] * len(token))  # Label 1 for facts\n",
    "            elif text in ruling_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([0] * len(token)) # Label 0 for rulings\n",
    "            elif text in issue_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([2] * len(token))  # Label 2 for issues\n",
    "\n",
    "        # flatten list\n",
    "        new_tokens = [token for sublist in new_tokens for token in sublist]\n",
    "        return labels, new_tokens\n",
    "\n",
    "    def get_finetune_data(self):\n",
    "        return self.finetune_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b2747-cc8d-4ff4-95c3-a38d32667614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CourtCaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the court case data. Expects data in the form of input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09b567-f9fe-4e73-bcdf-1d8bc95e53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa0edb-c143-4ea2-a9eb-473220c57ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prep_model:\n",
    "    def __init__(self, bert_tokenizer, bert_model):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.unknown_tokens = []\n",
    "        self.metric = evaluate.load(\"accuracy\")\n",
    "        self.training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                               eval_strategy=\"epoch\", \n",
    "                                               fp16=True, \n",
    "                                               learning_rate=3e-5,\n",
    "                                              per_device_train_batch_size=4)\n",
    "        self.trainer = None\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except FileNotFoundError:\n",
    "            print(\"No 'unknown_tokens.txt' file found. Proceeding without new tokens.\")\n",
    "\n",
    "        # Update tokenizer with loaded tokens\n",
    "        if self.unknown_tokens:\n",
    "            self.update_tokenizer()\n",
    "\n",
    "    def update_tokenizer(self):\n",
    "        # Add the new tokens to the tokenizer\n",
    "        self.tokenizer.add_tokens(self.unknown_tokens)\n",
    "\n",
    "        # Resize the model's token embeddings to match the new tokenizer length\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def prepare_datasets(self, finetune_data):\n",
    "        \"\"\"\n",
    "        Prepare training and evaluation datasets using train_test_split from sklearn.\n",
    "        \"\"\"\n",
    "        input_ids = [entry[\"input_ids\"] for entry in finetune_data]\n",
    "        attention_masks = [entry[\"attention_mask\"] for entry in finetune_data]\n",
    "        labels = [entry[\"labels\"] for entry in finetune_data]\n",
    "\n",
    "        # Split the data into training and evaluation sets (80% train, 20% eval)\n",
    "        train_input_ids, eval_input_ids, train_attention_masks, eval_attention_masks, train_labels, eval_labels = train_test_split(\n",
    "            input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Create custom PyTorch datasets\n",
    "        train_dataset = CourtCaseDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "        eval_dataset = CourtCaseDataset(eval_input_ids, eval_attention_masks, eval_labels)\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def finetune_model(self, finetune_data):\n",
    "        # Prepare datasets\n",
    "        train_dataset, eval_dataset = self.prepare_datasets(finetune_data)\n",
    "\n",
    "        # Define a trainer object\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=train_dataset,  \n",
    "            eval_dataset=eval_dataset,    \n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train Model\n",
    "        self.trainer.train()\n",
    "\n",
    "        # Save the fine-tuned model and tokenizer\n",
    "        self.model.save_pretrained(\"fine-tuned-legal-bert-model\")\n",
    "        self.tokenizer.save_pretrained(\"fine-tuned-legal-bert-tokenizer\")\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Flatten predictions and labels\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "        \n",
    "        # Filter out the ignored index (-100)\n",
    "        valid_indices = labels != -100\n",
    "        valid_predictions = predictions[valid_indices]\n",
    "        valid_labels = labels[valid_indices]\n",
    "\n",
    "        # Compute accuracy using the valid predictions and labels\n",
    "        return self.metric.compute(predictions=valid_predictions, references=valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f68d9-836d-4737-87b8-c23e98a20a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901cbc3-048f-48ca-aa31-8cabb6038887",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef199008-e603-4f25-9c8f-8be5863ccb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqtoseq_preprocessor = preprocess_seqtoseq_data(\"new_court_cases.csv\", tokenizer) #tokenizer still in discussion sa utak ko\n",
    "seqtoseq_data = seqtoseq_preprocessor.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b85477-362e-417d-8312-187ee4c73dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prep_model(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446b881-1cb1-4d4d-b93e-1b99f9bd52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_preprocessor = preprocess_finetuning_data(\"new_court_cases.csv\", y.tokenizer)\n",
    "finetune_data = finetune_preprocessor.get_finetune_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144b43b-cbb1-41c8-b9f5-bcbbcb6f48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_data = finetune_data[0:900]\n",
    "len(finetune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7d8d7-e579-4653-90c3-530042d6a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution\n",
    "from collections import Counter\n",
    "labels = [entry['labels'] for entry in finetune_data]\n",
    "flattened_labels = [label for sublist in labels for label in sublist if label != -100]\n",
    "print(Counter(flattened_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831af14-a17c-44eb-8f42-50bbd761d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.finetune_model(finetune_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228fc98-9c28-4f27-a715-0f531ca8d55c",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a300cf-8eeb-4fa4-a1d8-5a74b12765b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"fine-tuned-legal-bert-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-legal-bert-tokenizer\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_data = finetune_data[1]\n",
    "input_ids = torch.tensor([input_data['input_ids']])  # Wrap in list to make it a batch of size 1\n",
    "attention_mask = torch.tensor([input_data['attention_mask']])\n",
    "\n",
    "# Move data to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the logits (predictions before applying softmax)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Get tokens corresponding to input IDs\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "\n",
    "# Display tokens and their corresponding labels\n",
    "for token, label in zip(tokens, predictions[0]):  # predictions[0] since we have a batch of size 1\n",
    "    print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41507c1a-c7e2-4d33-a5f1-e8b4564caca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
