{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971cd455-2154-4122-8753-be7924814ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfcdfb4-8eb1-4e95-bf15-2e843d8243e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+|\\.(?![a-zA-Z0-9])\")\n",
    "        self.window_size = 128\n",
    "        self.max_token = 510\n",
    "        \n",
    "    def split_to_chunks_with_windows_with_attention_mask(self, tokens, labels=None):\n",
    "        \"\"\"\n",
    "        Court cases has long text while BERT only has 512 maximum tokens. \n",
    "        This function splits the long text into chunks with 512 tokens with sliding windows.\n",
    "        Add necessary padding if not equal to 512 tokens.\n",
    "        Add attention mask.\n",
    "\n",
    "            Args:\n",
    "                tokens: A list of tokens.\n",
    "\n",
    "            Returns:\n",
    "                chunks: A list of list of string with 512 tokens including BERT's special token ([CLS] [SEP])\n",
    "                chunk_labels: A list of lists, each containing labels corresponding to the tokens in chunks.\n",
    "                attention_masks: A list of list of 1s and 0s that portrays what places are masked (0)\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        chunk_labels = []\n",
    "        attention_masks = []\n",
    "\n",
    "        # Iterate over the list of tokens and chunking them with window slides\n",
    "        for i in range(0, len(tokens), self.max_token - self.window_size):\n",
    "            # store the current window of tokens and labels\n",
    "            chunk_tokens = tokens[i:i + self.max_token]\n",
    "            if labels:\n",
    "                chunk_label = labels[i:i + self.max_token]\n",
    "\n",
    "            # add special tokens\n",
    "            chunk_tokens.insert(0, \"[CLS]\")\n",
    "            chunk_tokens.append(\"[SEP]\")\n",
    "\n",
    "            if labels:\n",
    "                # Add corresponding labels for special tokens\n",
    "                chunk_label.insert(0, -100)  # Label for [CLS]\n",
    "                chunk_label.append(-100)     # Label for [SEP]\n",
    "\n",
    "            # Add padding and mask if less than 512 tokens\n",
    "            if len(chunk_tokens) < self.max_token + 2: # +2 for [CLS] and [SEP]\n",
    "                chunk_tokens, mask = self.pad_and_mask(chunk_tokens, maxlength=self.max_token + 2)\n",
    "                if labels:\n",
    "                    chunk_label.extend([-100] * (self.max_token + 2 - len(chunk_label)))  # Padding labels with -100 as well\n",
    "            else: # just add the mask\n",
    "                mask = np.ones(512, dtype=int).tolist()\n",
    "\n",
    "            # append\n",
    "            chunks.append(chunk_tokens)\n",
    "            attention_masks.append(mask)\n",
    "            if labels:\n",
    "                chunk_labels.append(chunk_label)\n",
    "\n",
    "            # Break loop if we have covered the entire sequence\n",
    "            if i + self.max_token >= len(tokens):\n",
    "                break\n",
    "\n",
    "        # If used for finetuning\n",
    "        if labels:\n",
    "            return chunks, chunk_labels, attention_masks\n",
    "        else:\n",
    "            return (chunks, attention_masks)\n",
    "\n",
    "    def pad_and_mask(self, chunk, maxlength=512):\n",
    "        \"\"\"\n",
    "        Add [PAD] tokens to the chunk to make its length equal to maxlength (512 tokens).\n",
    "        Add mask attention.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The list of tokens.\n",
    "            maxlength: The target length after padding (default is 512).\n",
    "        \n",
    "        Returns:\n",
    "            chunk: list of tokens with paddings.\n",
    "            attention_mask: list of 1s and 0s for masking the paddings.\n",
    "        \"\"\"\n",
    "        attention_mask = []\n",
    "        \n",
    "        # Calculate how many [PAD] tokens are needed\n",
    "        pads_to_add = maxlength - len(chunk)\n",
    "\n",
    "        # Add the attention mask\n",
    "        attention_mask = [1] * len(chunk) + [0] * pads_to_add\n",
    "    \n",
    "        # Extend the chunk with [PAD] tokens\n",
    "        chunk.extend([\"[PAD]\"] * pads_to_add)\n",
    "\n",
    "        return chunk, attention_mask\n",
    "\n",
    "    def change_char(self, text):\n",
    "        \"\"\"\n",
    "        Removes special characters from a list of strings using regular expressions.\n",
    "        As well as change characters.\n",
    "        \n",
    "        \n",
    "          Args:\n",
    "            strings: A list of strings.\n",
    "        \n",
    "          Returns:\n",
    "            A new list of strings without special characters.\n",
    "          \"\"\"\n",
    "        text = re.sub(r'[(),:;\\'\"’”[]]', '', text)\n",
    "        text = re.sub(r'rtc', 'regional trial court', text)\n",
    "        text = re.sub(r\"\\w*\\d+\\w*\", \"\", text)\n",
    "        text = re.sub(r\"“\", \"\", text)\n",
    "        text = re.sub(r\",”\", \"\", text)\n",
    "        text = re.sub(r\",\", \"\", text)\n",
    "        text = re.sub(r\",,.\", \"\", text)\n",
    "        text = re.sub(r\",,.,\", \"\", text)\n",
    "        text = re.sub(r\"--,\", \"\", text)\n",
    "        text = re.sub(r\"\\bno.\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\bg\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\br\\b\", \"number \", text)\n",
    "        text = re.sub(r\"\\u2033\", \"\", text)\n",
    "        text = re.sub(r\"\\u2032\", \"\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c884ae0-ead1-4616-b171-828f37e64824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_seqtoseq_data(preprocess):\n",
    "    def __init__(self, file_path, bert_tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.court_cases = None\n",
    "        self.rulings = None\n",
    "        self.issues = None\n",
    "        self.facts = None\n",
    "\n",
    "        # Flag for new tokens found\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "\n",
    "        # use this variable for debugging\n",
    "        self.debugging = True\n",
    "\n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # preprocess\n",
    "        self.preprocess()\n",
    "\n",
    "        # drop duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "    def preprocess(self):\n",
    "        # lowercase the text and Remove unnecessary characters\n",
    "        self.court_cases = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.rulings = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.facts = [self.change_char(text.lower())for text in self.df[\"facts\"]]\n",
    "        self.issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "\n",
    "        # tokenize the text, storing words only\n",
    "        self.court_cases = [self.tokenizer.tokenize(text) for text in self.court_cases]\n",
    "        self.rulings = [self.tokenizer.tokenize(text) for text in self.rulings]\n",
    "        self.facts = [self.tokenizer.tokenize(text) for text in self.facts]\n",
    "        self.issues = [self.tokenizer.tokenize(text) for text in self.issues]\n",
    "        \n",
    "        # if longer than 512 tokens, chunk the tokens into 512 while adding windows and paddings & attention mask\n",
    "        self.court_cases = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.court_cases]\n",
    "        self.rulings = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.rulings]\n",
    "        self.facts = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.facts]\n",
    "        self.issues = [self.split_to_chunks_with_windows_with_attention_mask(tokens) for tokens in self.issues]\n",
    "\n",
    "    def prepare_input_output(self, chunks):\n",
    "        \"\"\"\n",
    "        Prepare input-output pairs for each chunk. \n",
    "        Returns a list of tuples, where each tuple represents an (input, output) pair.\n",
    "        \"\"\"\n",
    "        input_output_pairs = []\n",
    "        for chunk in chunks:\n",
    "            # Tokenize the chunk and convert to IDs\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(chunk)\n",
    "    \n",
    "            # Verify that the chunk ends with the [SEP] token to avoid duplicates\n",
    "            not_sep = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "            not_pad = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "            if not_sep and not_pad:\n",
    "                input_ids.append(self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "    \n",
    "            # Prepare the shifted output (excluding the initial [CLS] token)\n",
    "            shifted_output = input_ids[1:]  # Shifted output starts from the second token\n",
    "    \n",
    "            # No need to append [SEP] here, it's already included in input_ids if required\n",
    "            # Add input-output pair to list\n",
    "            input_output_pairs.append((input_ids, shifted_output))\n",
    "\n",
    "            # Check for unknown tokens and append them to the list that will be added\n",
    "            unk_tokens = chunk\n",
    "            for i in range(len(unk_tokens)):\n",
    "                if input_ids[i] == 100 and unk_tokens[i] not in self.unknown_tokens:\n",
    "                    print(unk_tokens[i],\" : \",input_ids[i])\n",
    "                    self.found_new_unknown_token = True\n",
    "                    self.unknown_tokens.append(unk_tokens[i])\n",
    "\n",
    "            '''if self.debugging == True:\n",
    "                self.debugging = False\n",
    "                print(\"output of the decoder: \",shifted_output)\n",
    "                print(\"input to the decoder:\",input_ids)'''\n",
    "                        \n",
    "            \n",
    "        return input_output_pairs\n",
    "\n",
    "    def prepare_court_case(self, chunks):\n",
    "        \"\"\"\n",
    "        Convert court cases tokens into their respective IDs.\n",
    "\n",
    "            Args:\n",
    "                chunks: A list of tokens.\n",
    "        \n",
    "            Returns:\n",
    "                \"court_cases_ids\", a list of of list of IDs (integers).\n",
    "        \"\"\"\n",
    "        court_cases_ids = []\n",
    "        for chunk in chunks:\n",
    "            # Tokenize the chunk and convert to IDs\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(chunk)\n",
    "    \n",
    "            # Verify that the chunk ends with the [SEP] token to avoid duplicates\n",
    "            not_sep = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "            not_pad = input_ids[-1] != self.bert_tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "            if not_sep and not_pad:\n",
    "                input_ids.append(self.bert_tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
    "\n",
    "            court_cases_ids.append(input_ids)\n",
    "                \n",
    "        return court_cases_ids\n",
    "\n",
    "    def get_training_data(self):\n",
    "        \"\"\"\n",
    "        Prepare training data for all segments, maintaining the structure per court case.\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        self.found_new_unknown_token = False\n",
    "        \n",
    "        for i in range(len(self.court_cases)):\n",
    "            # Prepare input-output pairs for each segment within a single court case\n",
    "            court_case_data = self.prepare_court_case(self.court_cases[i][0])\n",
    "            ruling_data = self.prepare_input_output(self.rulings[i][0])\n",
    "            fact_data = self.prepare_input_output(self.facts[i][0])\n",
    "            issue_data = self.prepare_input_output(self.issues[i][0])\n",
    "            \n",
    "            # Maintain structure by grouping segments within the same court case\n",
    "            case_data = {\n",
    "                \"court_case\": court_case_data,\n",
    "                \"rulings\": ruling_data,\n",
    "                \"facts\": fact_data,\n",
    "                \"issues\": issue_data\n",
    "            }\n",
    "\n",
    "            '''print(type(ruling_data))\n",
    "            print(type(ruling_data[0]))\n",
    "            print(type(ruling_data[0][0]))'''\n",
    "            \n",
    "            training_data.append(case_data)\n",
    "\n",
    "        # If unknown token/s found, Update file containing all unknown token & Raise an error message\n",
    "        if self.unknown_tokens and self.found_new_unknown_token:\n",
    "            with open('unknown_tokens.txt', 'w') as f:\n",
    "                for token in self.unknown_tokens:\n",
    "                    f.write(f\"{token}\\n\")\n",
    "            raise Exception(\"There are unknown token/s found. Update the tokenizer and finetune the model.\")\n",
    "        \n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b631f11d-f06c-48c4-a6c5-634bb3eb2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_finetuning_data(preprocess):\n",
    "    def __init__(self, file_path, bert_tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "        # Flag for new tokens found\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "    \n",
    "        # For fine-tuning\n",
    "        self.finetune_court = None\n",
    "        self.finetune_ruling = None\n",
    "        self.finetune_issues = None\n",
    "        self.finetune_facts = None\n",
    "        self.finetune_data = []\n",
    "\n",
    "        # drop null values in the comment\n",
    "        self.df.dropna(inplace=True)\n",
    "        \n",
    "        # preprocess and prepare proper format of data for finetuning\n",
    "        self.preprocess()\n",
    "        self.prepare_finetune_data()\n",
    "\n",
    "        # drop duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "         \n",
    "    def preprocess(self):\n",
    "        # lowercase the text and Remove unnecessary characters\n",
    "        self.finetune_court = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.finetune_ruling = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.finetune_facts = [self.change_char(text.lower())for text in self.df[\"facts\"]]\n",
    "        self.finetune_issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "\n",
    "        # tokenize the text, accepting words, numbers, and dots only\n",
    "        self.finetune_court = [self.tokenizer.tokenize(text) for text in self.finetune_court]\n",
    "        self.finetune_ruling = [self.tokenizer.tokenize(text) for text in self.finetune_ruling]\n",
    "        self.finetune_facts = [self.tokenizer.tokenize(text) for text in self.finetune_facts]\n",
    "        self.finetune_issues = [self.tokenizer.tokenize(text) for text in self.finetune_issues]\n",
    "\n",
    "        # convert to whole string\n",
    "        self.finetune_court = [' '.join(token) for token in self.finetune_court]\n",
    "        self.finetune_ruling = [' '.join(token) for token in self.finetune_ruling]\n",
    "        self.finetune_facts = [' '.join(token) for token in self.finetune_facts]\n",
    "        self.finetune_issues = [' '.join(token) for token in self.finetune_issues]\n",
    "\n",
    "        # split into tokens (sentences)\n",
    "        self.finetune_court = [sent_tokenize(text) for text in self.finetune_court]\n",
    "        self.finetune_ruling = [sent_tokenize(text) for text in self.finetune_ruling]\n",
    "        self.finetune_facts = [sent_tokenize(text) for text in self.finetune_facts]\n",
    "        self.finetune_issues = [sent_tokenize(text) for text in self.finetune_issues]\n",
    "            \n",
    "    def prepare_finetune_data(self):\n",
    "        \"\"\"\n",
    "        Prepare token classification data, labeling each token in the court case with\n",
    "        its corresponding segment (rulings, facts, or issues).\n",
    "\n",
    "        Variables:\n",
    "            tokens_and_labels: list of tuples, wherein each tuples contains the list of 512 tokens and the list of labels of those 512 tokens.\n",
    "            case_labels: list of integers, wherein each integer corresponds to a court case segment (i.e. rulings = 0).\n",
    "            new_tokens: list of tokens, wherein each token is a sentence of a corresponding segment label.\n",
    "            chunks:\n",
    "            chunk_labels:\n",
    "            attention_masks:\n",
    "        \"\"\"\n",
    "        for i in range(len(self.finetune_court)):\n",
    "            # Create context-based labels for the entire court case\n",
    "            case_labels, new_tokens = self.prepare_contextual_labels(self.finetune_court[i], self.finetune_ruling[i], \n",
    "                                                         self.finetune_facts[i], self.finetune_issues[i])\n",
    "\n",
    "            chunks, chunk_labels, attention_masks = self.split_to_chunks_with_windows_with_attention_mask(new_tokens, case_labels)\n",
    "\n",
    "            for i in range(len(chunks)):\n",
    "                self.finetune_data.append({\n",
    "                    \"input_ids\": self.bert_tokenizer.convert_tokens_to_ids(chunks[i]),\n",
    "                    \"labels\": chunk_labels[i],\n",
    "                    \"attention_mask\": attention_masks[i]\n",
    "                })\n",
    "\n",
    "    def prepare_contextual_labels(self, case_tokens, ruling_tokens, fact_tokens, issue_tokens):\n",
    "        \"\"\"\n",
    "        Given the tokenized court case, assign labels to each token based on context.\n",
    "\n",
    "            Returns:\n",
    "                labels: list of integers corresponding to their court casesegment label\n",
    "                new_tokens: list of tokens, wherein each tokens is a sentence converted to word format.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        new_tokens = []\n",
    "        \n",
    "        for text in case_tokens:\n",
    "            token = self.tokenizer.tokenize(text)\n",
    "            if text in fact_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([1] * len(token))  # Label 1 for facts\n",
    "            elif text in ruling_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([0] * len(token)) # Label 0 for rulings\n",
    "            elif text in issue_tokens:\n",
    "                new_tokens.append(token)\n",
    "                labels.extend([2] * len(token))  # Label 2 for issues\n",
    "\n",
    "        # flatten list\n",
    "        new_tokens = [token for sublist in new_tokens for token in sublist]\n",
    "        return labels, new_tokens\n",
    "\n",
    "    def get_finetune_data(self):\n",
    "        return self.finetune_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369b2747-cc8d-4ff4-95c3-a38d32667614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CourtCaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the court case data. Expects data in the form of input_ids, attention_mask, and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e09b567-f9fe-4e73-bcdf-1d8bc95e53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(outputs.logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcaa0edb-c143-4ea2-a9eb-473220c57ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prep_model:\n",
    "    def __init__(self, bert_tokenizer, bert_model):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.unknown_tokens = []\n",
    "        self.metric = evaluate.load(\"accuracy\")\n",
    "        self.training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                               eval_strategy=\"epoch\", \n",
    "                                               fp16=True, \n",
    "                                               learning_rate=3e-5,\n",
    "                                              per_device_train_batch_size=4)\n",
    "        self.trainer = None\n",
    "\n",
    "        # Load the unknown tokens\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except FileNotFoundError:\n",
    "            print(\"No 'unknown_tokens.txt' file found. Proceeding without new tokens.\")\n",
    "\n",
    "        # Update tokenizer with loaded tokens\n",
    "        if self.unknown_tokens:\n",
    "            self.update_tokenizer()\n",
    "\n",
    "    def update_tokenizer(self):\n",
    "        # Add the new tokens to the tokenizer\n",
    "        self.tokenizer.add_tokens(self.unknown_tokens)\n",
    "\n",
    "        # Resize the model's token embeddings to match the new tokenizer length\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def prepare_datasets(self, finetune_data):\n",
    "        \"\"\"\n",
    "        Prepare training and evaluation datasets using train_test_split from sklearn.\n",
    "        \"\"\"\n",
    "        input_ids = [entry[\"input_ids\"] for entry in finetune_data]\n",
    "        attention_masks = [entry[\"attention_mask\"] for entry in finetune_data]\n",
    "        labels = [entry[\"labels\"] for entry in finetune_data]\n",
    "\n",
    "        # Split the data into training and evaluation sets (80% train, 20% eval)\n",
    "        train_input_ids, eval_input_ids, train_attention_masks, eval_attention_masks, train_labels, eval_labels = train_test_split(\n",
    "            input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Create custom PyTorch datasets\n",
    "        train_dataset = CourtCaseDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "        eval_dataset = CourtCaseDataset(eval_input_ids, eval_attention_masks, eval_labels)\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def finetune_model(self, finetune_data, flat_labels, device):\n",
    "        # Compute class weights\n",
    "        self.class_weights = compute_class_weight('balanced', classes=np.array([0, 1, 2]), y=flat_labels)\n",
    "        self.class_weights = torch.tensor(self.class_weights, dtype=torch.float).to(device)  # Move to GPU if using CUDA\n",
    "    \n",
    "        # Prepare datasets\n",
    "        train_dataset, eval_dataset = self.prepare_datasets(finetune_data)\n",
    "    \n",
    "        # Define a custom trainer object\n",
    "        self.trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=train_dataset,  \n",
    "            eval_dataset=eval_dataset,    \n",
    "            compute_metrics=self.compute_metrics,\n",
    "            class_weights=self.class_weights  # Pass the class weights\n",
    "        )\n",
    "        \n",
    "        # Train Model\n",
    "        self.trainer.train()\n",
    "    \n",
    "        # Save the fine-tuned model and tokenizer\n",
    "        self.model.save_pretrained(\"fine-tuned-legal-bert-model\")\n",
    "        self.tokenizer.save_pretrained(\"fine-tuned-legal-bert-tokenizer\")\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Flatten predictions and labels\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "        \n",
    "        # Filter out the ignored index (-100)\n",
    "        valid_indices = labels != -100\n",
    "        valid_predictions = predictions[valid_indices]\n",
    "        valid_labels = labels[valid_indices]\n",
    "\n",
    "        # Compute accuracy using the valid predictions and labels\n",
    "        return self.metric.compute(predictions=valid_predictions, references=valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12f68d9-836d-4737-87b8-c23e98a20a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9901cbc3-048f-48ca-aa31-8cabb6038887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef199008-e603-4f25-9c8f-8be5863ccb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqtoseq_preprocessor = preprocess_seqtoseq_data(\"new_court_cases.csv\", tokenizer) #tokenizer still in discussion sa utak ko\n",
    "seqtoseq_data = seqtoseq_preprocessor.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b85477-362e-417d-8312-187ee4c73dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prep_model(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1446b881-1cb1-4d4d-b93e-1b99f9bd52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_preprocessor = preprocess_finetuning_data(\"new_court_cases.csv\", y.tokenizer)\n",
    "finetune_data = finetune_preprocessor.get_finetune_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d144b43b-cbb1-41c8-b9f5-bcbbcb6f48fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_data = finetune_data[0:900]\n",
    "len(finetune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec7d8d7-e579-4653-90c3-530042d6a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 338679, 1: 88435, 2: 20841})\n"
     ]
    }
   ],
   "source": [
    "# check class distribution\n",
    "labels = [entry['labels'] for entry in finetune_data]\n",
    "flattened_labels = [label for sublist in labels for label in sublist if label != -100]\n",
    "print(Counter(flattened_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1831af14-a17c-44eb-8f42-50bbd761d311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [181/540 1:37:52 < 3:16:18, 0.03 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "CustomTrainer.compute_loss() got an unexpected keyword argument 'return_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetune_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 70\u001b[0m, in \u001b[0;36mprep_model.finetune_model\u001b[1;34m(self, finetune_data, flat_labels, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m     61\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     62\u001b[0m     args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights  \u001b[38;5;66;03m# Pass the class weights\u001b[39;00m\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tuned-legal-bert-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:4061\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4058\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4060\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4061\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4062\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4063\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:4279\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[0;32m   4278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4279\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   4280\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   4282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: CustomTrainer.compute_loss() got an unexpected keyword argument 'return_outputs'"
     ]
    }
   ],
   "source": [
    "y.finetune_model(finetune_data, flattened_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228fc98-9c28-4f27-a715-0f531ca8d55c",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a300cf-8eeb-4fa4-a1d8-5a74b12765b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"fine-tuned-legal-bert-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-legal-bert-tokenizer\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_data = finetune_data[1]\n",
    "input_ids = torch.tensor([input_data['input_ids']])  # Wrap in list to make it a batch of size 1\n",
    "attention_mask = torch.tensor([input_data['attention_mask']])\n",
    "\n",
    "# Move data to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get the logits (predictions before applying softmax)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Get tokens corresponding to input IDs\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "\n",
    "# Display tokens and their corresponding labels\n",
    "for token, label in zip(tokens, predictions[0]):  # predictions[0] since we have a batch of size 1\n",
    "    print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41507c1a-c7e2-4d33-a5f1-e8b4564caca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
