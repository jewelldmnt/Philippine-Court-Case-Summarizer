{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe31d63-600e-40f5-b0a9-475567e5787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from transformers import Trainer, TrainingArguments, LEDTokenizer, LEDForConditionalGeneration, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59f034e-1eaf-4a4a-8973-5ca5f169f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, file_path):\n",
    "        # Tokenization and cleaning related variable\n",
    "        self.regex_tokenizer = RegexpTokenizer(r\"[a-zA-Z0-9]+|\\.(?![a-zA-Z0-9])\")\n",
    "        self.stopwords = ['the','of','to']\n",
    "        \n",
    "        # Model related variables\n",
    "        self.encoder_max_token = 4096\n",
    "        self.decoder_max_token = 1024\n",
    "        self.LED_tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "        self.LED_model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "        self.global_mask = []\n",
    "\n",
    "        # Data related variables\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.court_cases = []\n",
    "        self.rulings = []\n",
    "        self.issues = []\n",
    "        self.facts = []\n",
    "        self.led_court = []\n",
    "\n",
    "        # Actual Data input to the model\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_rulings = []\n",
    "        self.decoder_issues = []\n",
    "        self.decoder_facts = []\n",
    "        self.global_mask = []\n",
    "        self.final_data = []\n",
    "\n",
    "        # Unknown token variables\n",
    "        try:\n",
    "            with open('unknown_tokens.txt', 'r') as f:\n",
    "                self.unknown_tokens = f.read().splitlines()\n",
    "        except:\n",
    "            self.unknown_tokens = []\n",
    "        self.found_new_unknown_token = False\n",
    "\n",
    "        # Preprocess and prepare raw data\n",
    "        self.df.dropna(inplace=True)\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        self.preprocess()\n",
    "        self.find_unknown_token()\n",
    "        if self.found_new_unknown_token:\n",
    "            self.add_tokens_tokenizer_model()\n",
    "        self.add_globalmask()\n",
    "        self.prepare_LED_data()\n",
    "\n",
    "    def return_model_tokenizer_data(self):\n",
    "        return self.LED_model, self.LED_tokenizer, self.final_data\n",
    "\n",
    "    def return_visualization_data(self):\n",
    "        return self.court_cases, self.rulings, self.issues, self.facts\n",
    "\n",
    "    def prepare_LED_data(self):\n",
    "        # length of encoder and decoder inputs are the same and should be a list of strings\n",
    "        for i in range(len(self.encoder_inputs)):\n",
    "            # Tokenize the input (court case text)\n",
    "            inputs = self.LED_tokenizer(self.encoder_inputs[i], \n",
    "                               max_length=self.encoder_max_token, \n",
    "                               padding=\"max_length\", \n",
    "                               truncation=True, \n",
    "                               return_tensors=\"pt\")\n",
    "            \n",
    "            # Tokenize the issues (segmentation)\n",
    "            issues_output = self.LED_tokenizer(self.decoder_issues[i], \n",
    "                                max_length=self.decoder_max_token,\n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "            # Tokenize the issues (segmentation)\n",
    "            facts_output = self.LED_tokenizer(self.decoder_facts[i], \n",
    "                                max_length=self.decoder_max_token,\n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "            # Tokenize the issues (segmentation)\n",
    "            ruling_output = self.LED_tokenizer(self.decoder_rulings[i], \n",
    "                                max_length=self.decoder_max_token,\n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "            # prepare court case input ids\n",
    "            input_ids = inputs.input_ids\n",
    "            attention_mask = inputs.attention_mask\n",
    "\n",
    "            # prepare decoder input ids\n",
    "            issues_input_ids = issues_output.input_ids.clone()\n",
    "            facts_input_ids = facts_output.input_ids.clone()\n",
    "            ruling_input_ids = ruling_output.input_ids.clone()\n",
    "\n",
    "            # ensure global_attention_mask is padded to the correct length\n",
    "            global_attention_mask = self.global_mask[i]\n",
    "            if len(global_attention_mask) < 4096:\n",
    "                padding_length = 4096 - len(global_attention_mask)\n",
    "                global_attention_mask = global_attention_mask + [0] * padding_length\n",
    "    \n",
    "            global_attention_mask = torch.tensor(global_attention_mask).unsqueeze(0)  # convert to tensor and match input shape\n",
    "            '''print('court: ',len(input_ids[0]))\n",
    "            print('issues: ',len(issues[0]))\n",
    "            print('facts: ',len(facts[0]))\n",
    "            print('ruling: ',len(ruling[0]))\n",
    "            print('global_attention_mask: ',len(global_attention_mask[0]))'''  # debugging here\n",
    "\n",
    "            # prepare final data structure\n",
    "            data = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"global_attention_mask\": global_attention_mask,\n",
    "                \"issues_input_ids\": issues_input_ids,\n",
    "                \"facts_input_ids\": facts_input_ids,\n",
    "                \"ruling_input_ids\": ruling_input_ids\n",
    "            }\n",
    "\n",
    "            # append final data\n",
    "            self.final_data.append(data)\n",
    "        \n",
    "        \n",
    "    def add_globalmask(self):\n",
    "        '''\n",
    "        Add global mask to the first tokens of the start of each segment.\n",
    "        '''\n",
    "        # Prepare encoder/decoder inputs and global attention mask\n",
    "        current_label = ''\n",
    "        for i in range(len(self.court_cases)):\n",
    "            # instantiate lists for encoder, decoder and attention masks\n",
    "            list_encoder = []\n",
    "            list_issues = []\n",
    "            list_facts = []\n",
    "            list_ruling = []\n",
    "            list_attntn = []\n",
    "    \n",
    "            for sentence in self.court_cases[i]:  # process each sentence one by one\n",
    "                # Tokenize each sentence\n",
    "                token = self.regex_tokenizer.tokenize(sentence)\n",
    "                token_ids = self.LED_tokenizer.convert_tokens_to_ids(token)  # Convert to token IDs for LED input\n",
    "                global_attntn = [0] * len(token_ids)  # Initialize global attention for each sentence\n",
    "\n",
    "                if sentence in self.issues[i]:\n",
    "                    list_encoder.append(sentence)\n",
    "                    list_issues.append(sentence)\n",
    "    \n",
    "                    if current_label != \"<ISSUES>\" and len(token) >= 7:\n",
    "                        # Add special token and set global attention for the first 3 tokens\n",
    "                        global_attntn = [1] * 2 + [0] * (len(token) - 2)\n",
    "                    else:\n",
    "                        global_attntn = [0] * len(token)\n",
    "                    # Append to decoder inputs and global mask\n",
    "                    list_attntn.append(global_attntn)\n",
    "                    current_label = \"<ISSUES>\"\n",
    "    \n",
    "                if sentence in self.facts[i]:\n",
    "                    list_encoder.append(sentence)\n",
    "                    list_facts.append(sentence)\n",
    "    \n",
    "                    if current_label != \"<FACTS>\" and len(token) >= 7:\n",
    "                        # Add special token and set global attention for the first 3 tokens\n",
    "                        global_attntn = [1] * 3 + [0] * (len(token) - 3)\n",
    "                    else:\n",
    "                        global_attntn = [0] * len(token)\n",
    "                    # Append to decoder inputs and global mask\n",
    "                    list_attntn.append(global_attntn)\n",
    "                    current_label = \"<FACTS>\"\n",
    "    \n",
    "                elif sentence in self.rulings[i]:\n",
    "                    list_encoder.append(sentence)\n",
    "                    list_ruling.append(sentence)\n",
    "    \n",
    "                    if current_label != \"<RULING>\" and len(token) >= 7:\n",
    "                        # Add special token and set global attention for the first 3 tokens\n",
    "                        global_attntn = [1] * 4 + [0] * (len(token) - 4)\n",
    "                    else:\n",
    "                        global_attntn = [0] * len(token)\n",
    "                    # Append to decoder inputs and global mask\n",
    "                    list_attntn.append(global_attntn)\n",
    "                    current_label = \"<RULING>\"\n",
    "\n",
    "            '''x = ' '.join(list_encoder)\n",
    "            x = self.regex_tokenizer.tokenize(x)\n",
    "            y = [attn for attn_list in list_attntn for attn in attn_list]''' # debugging here\n",
    "\n",
    "            # Concatenate and append strings and attention\n",
    "            self.encoder_inputs.append(' '.join(list_encoder))  # Combine encoder segments\n",
    "            self.decoder_rulings.append(' '.join(list_ruling))  # Combine ruling segments\n",
    "            self.decoder_issues.append(' '.join(list_issues))   # Combine issue segments\n",
    "            self.decoder_facts.append(' '.join(list_facts))     # Combine fact segments\n",
    "            self.global_mask.append([attn for attn_list in list_attntn for attn in attn_list])  # Flatten global attention\n",
    "\n",
    "\n",
    "                \n",
    "    def add_tokens_tokenizer_model(self):\n",
    "        '''\n",
    "        Add unknown token including special tokens to the tokenizer and resize the token embedding of the model.\n",
    "        '''\n",
    "        # Add special tokens if not in tokenizer\n",
    "        special_tokens = ['<RULING>', '<ISSUES>', '<FACTS>']\n",
    "        if not set(special_tokens).issubset(set(self.unknown_tokens)):\n",
    "            self.unknown_tokens.extend(special_tokens)\n",
    "                    \n",
    "        # Add the new tokens to the tokenizer\n",
    "        self.tokenizer.add_tokens(self.unknown_tokens)\n",
    "\n",
    "        # Resize the model's token embeddings to match the new tokenizer length\n",
    "        self.LED_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def find_unknown_token(self):\n",
    "        '''\n",
    "        Iterate over each cases, converting the tokens into IDs. Check if there is an unknown token in input_ids\n",
    "        '''\n",
    "        for case_tokens in self.court_cases:\n",
    "            input_ids = self.LED_tokenizer.convert_tokens_to_ids(case_tokens)\n",
    "            for input_id in input_ids:\n",
    "                if input_ids == 100 and input_ids not in self.unknown_tokens:\n",
    "                    print(case_tokens[i],\" : \",input_ids[i])\n",
    "                    self.found_new_unknown_token = True\n",
    "                    self.unknown_tokens.append(case_tokens[i])\n",
    "\n",
    "    def preprocess(self):\n",
    "        '''\n",
    "        Clean characters and tokenize court cases and segments.\n",
    "        '''\n",
    "        # Lowercase the text and Remove unnecessary characters\n",
    "        self.court_cases = [self.change_char(text.lower()) for text in self.df[\"whole_text\"]]\n",
    "        self.rulings = [self.change_char(text.lower()) for text in self.df[\"ruling\"]]\n",
    "        self.facts = [self.change_char(text.lower()) for text in self.df[\"facts\"]]\n",
    "        self.issues = [self.change_char(text.lower()) for text in self.df[\"issues\"]]\n",
    "        \n",
    "        # Tokenize the text, storing words and numbers only\n",
    "        self.court_cases = [self.regex_tokenizer.tokenize(text) for text in self.court_cases]\n",
    "        self.rulings = [self.regex_tokenizer.tokenize(text) for text in self.rulings]\n",
    "        self.facts = [self.regex_tokenizer.tokenize(text) for text in self.facts]\n",
    "        self.issues = [self.regex_tokenizer.tokenize(text) for text in self.issues]\n",
    "\n",
    "        # Remove stopwords\n",
    "        self.court_cases = [self.removestop(token_list) for token_list in self.court_cases]\n",
    "        self.rulings = [self.removestop(token_list) for token_list in self.rulings]\n",
    "        self.facts = [self.removestop(token_list) for token_list in self.facts]\n",
    "        self.issues = [self.removestop(token_list) for token_list in self.issues]\n",
    "\n",
    "        # Add a filter to accept only lists with tokens lower than 8192\n",
    "        indices_to_remove = []\n",
    "        for i in range(len(self.court_cases)):\n",
    "            if len(self.court_cases[i]) > 8192:\n",
    "                indices_to_remove.append(i)\n",
    "        \n",
    "        # Remove elements from all lists based on indices_to_remove\n",
    "        for i in reversed(indices_to_remove):\n",
    "            self.court_cases.pop(i)\n",
    "            self.rulings.pop(i)\n",
    "            self.facts.pop(i)\n",
    "            self.issues.pop(i)\n",
    "\n",
    "        # Join tokens to form full strings for each case\n",
    "        self.court_cases = [' '.join(token) for token in self.court_cases]\n",
    "        self.rulings = [' '.join(token) for token in self.rulings]\n",
    "        self.facts = [' '.join(token) for token in self.facts]\n",
    "        self.issues = [' '.join(token) for token in self.issues]\n",
    "    \n",
    "        # Split into sentences\n",
    "        self.court_cases = [sent_tokenize(text) for text in self.court_cases] # text is the whole court case\n",
    "        self.rulings = [sent_tokenize(text) for text in self.rulings]\n",
    "        self.facts = [sent_tokenize(text) for text in self.facts]\n",
    "        self.issues = [sent_tokenize(text) for text in self.issues]\n",
    "\n",
    "    def removestop(self, token_list):\n",
    "        \"\"\"\n",
    "        Remove stopwords from a list of tokens. Handles case where stopwords are in lower case.\n",
    "        \"\"\"\n",
    "        return [token for token in token_list if token.lower() not in self.stopwords]\n",
    "\n",
    "    def change_char(self, text):\n",
    "        \"\"\"\n",
    "        Cleans up text by removing or replacing certain characters and patterns.\n",
    "    \n",
    "        Args:\n",
    "            text: A string to be cleaned.\n",
    "    \n",
    "        Returns:\n",
    "            A cleaned version of the string.\n",
    "        \"\"\"\n",
    "        # More specific substitutions first\n",
    "        text = re.sub(r\"\\bno\\.\\b\", \"number \", text, flags=re.IGNORECASE)  # Replace \"no.\" with \"number\"\n",
    "        text = re.sub(r\"r.a.\", \"ra \", text, flags=re.IGNORECASE)    # Replace \"R.A.\" with \"ra\"\n",
    "        text = re.sub(r\"section (\\d+)\\.\", r\"section \\1\", text) # Replace \"section N.\" with \"section N\" where N is a number\n",
    "        text = re.sub(r\"sec.\", r\"sec\", text) # Replace \"section N.\" with \"section N\" where N is a number\n",
    "        text = re.sub(r\"p.d.\", r\"pd\", text) # Replace \".d.\" with \"pd\" where N is a number\n",
    "        text = re.sub(r\"no.\", r\"number\", text) # Replace \"no.\" to \"number\"\n",
    "        text = re.sub(r\"\\brtc\\b\", \"regional trial court\", text)  # Replace \"rtc\" with \"regional trial court\"\n",
    "        \n",
    "        # Remove unwanted characters, but keep periods at the end of words\n",
    "        text = re.sub(r\"[(),'\\\"’”\\[\\]]\", \" \", text)   # Remove specific punctuation\n",
    "    \n",
    "        # Remove stray or special characters\n",
    "        text = re.sub(r\"[“”]\", \" \", text)              # Remove “ and ”\n",
    "        text = re.sub(r\"\\u2033\", \" \", text)            # Remove double prime (″)\n",
    "        text = re.sub(r\"\\u2032\", \" \", text)            # Remove prime (′)\n",
    "        \n",
    "        # Remove specific meaningless characters\n",
    "        text = re.sub(r\"\\bg\\b\", \" \", text)\n",
    "        text = re.sub(r\"\\br\\b\", \" \", text)\n",
    "    \n",
    "        # Return cleaned text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fbae39a-859c-4f5a-906d-1ceaad4989b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "299\n",
      "299\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data, model, and tokenizer before training\n",
    "preprocessor = preprocess('new_court_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21206e33-2687-4dea-bdb9-01b31a218c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
