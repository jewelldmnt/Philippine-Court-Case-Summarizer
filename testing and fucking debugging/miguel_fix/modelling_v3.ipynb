{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55944bb4-9e10-495d-993d-0c4f5c76bd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_v3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60c48f8-fec6-4e07-a077-a04e17d6a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AdamW\n",
    "import time\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76bb581-ddcd-4a8b-84fd-ed801259fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # Tokenize the inputs\n",
    "    inputs = tokenizer(\n",
    "        batch[\"encoder_input_string\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "    \n",
    "    # Prepare input IDs and attention masks\n",
    "    batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
    "\n",
    "    labels = []\n",
    "    global_attention_mask = []\n",
    "    for i in range(len(batch[\"input_ids\"])):\n",
    "        # Prepare labels\n",
    "        labels.append(batch[\"segment_label\"][i])\n",
    "        print(f\"this is the {i}th batch: \", batch[\"segment_label\"][i])\n",
    "        \n",
    "        # Ensure global attention mask is padded to 1024 tokens\n",
    "        token_len = len(batch[\"input_ids\"][i]) - 1\n",
    "        global_attention_mask.append([1] + [0] * token_len)\n",
    "    \n",
    "    # Convert global attention mask to tensor\n",
    "    batch[\"global_attention_mask\"] = torch.tensor(global_attention_mask, dtype=torch.long)\n",
    "    batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    print('global: ',batch[\"global_attention_mask\"].shape)\n",
    "    print('labels: ',batch[\"labels\"].shape)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)  # Get the predicted class (highest score)\n",
    "\n",
    "    # Compute accuracy, precision, recall, and F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6567bb-f002-4d4a-8c11-c634ab16c66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\led\\modeling_led.py:2496: FutureWarning: The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of Transformers. No actual method were provided in the original paper on how to perfom sequence classification.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 0 new tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data, model, and tokenizer before training\n",
    "preprocessor = preprocess('court_cases_headings_labels.csv')\n",
    "model, tokenizer, xdata, ydata = preprocessor.return_model_tokenizer_data()\n",
    "xdata = xdata[:10]\n",
    "ydata = ydata[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7145ec9-b2df-4deb-9700-bf52b9bcb19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input_string': '4 batolacongan d . abdullah abdullah director finance budget and management services',\n",
       " 'segment_label': 0,\n",
       " '__index_level_0__': 15191}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to huggingface Dataset\n",
    "train_data = Dataset.from_pandas(xdata)\n",
    "eval_data = Dataset.from_pandas(ydata)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20993d9d-3f50-465b-aaaa-5703dc2e5492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f7940-9215-4fe6-8c5e-1b16112349b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0464abec4c7743369894c891060199b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618e3f26135143c5a2918c28eb14bd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map datasets to the model's expected input format\n",
    "train_dataset = train_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    remove_columns=[\"encoder_input_string\", \"segment_label\"]\n",
    ")\n",
    "\n",
    "eval_dataset = eval_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    remove_columns=[\"encoder_input_string\", \"segment_label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a75079a-9505-43c0-b82d-1b710f8b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf77d5a-0e43-4a41-9417-b34514384984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53146, 1024])\n",
      "torch.Size([53146, 1024])\n",
      "torch.Size([53146, 1024])\n",
      "torch.Size([53146])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['input_ids'].shape)\n",
    "print(train_dataset['global_attention_mask'].shape)\n",
    "print(train_dataset['attention_mask'].shape)\n",
    "print(train_dataset['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c45fd20-c74f-48b9-a677-3a34aa768521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_encoder_position_embeddings)\n",
    "print(model.config.max_decoder_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf7be74-d1a4-4240-b141-bc6dafaceed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEDConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 1024,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "53146\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(len(xdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1070b64-eb76-4273-ad10-910704eaa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=5,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,  # Optional: Enable mixed precision training if using GPU\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a30e6-0e7f-4273-9321-12872d267595",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be335d82-3715-40dc-9a25-fa7ce026c15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
