{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bb814a-c5c7-4796-aae8-7d7000dda8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TopicSegmentation import LegalBert, ModifiedStandardDecoder, PaddingMaskLayer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bf1ed-f6f4-424d-abc9-7db60b5fa6d3",
   "metadata": {},
   "source": [
    "# Instantiate Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1b1e0f-48a2-490b-a3f0-a67ae57af762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Segment_Input_Layer           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ Context_Vector_Input_Layer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ modified_standard_decoder_1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31000</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">87,010,328</span> │ Segment_Input_Layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ModifiedStandardDecoder</span>)     │                           │                 │ Context_Vector_Input_Laye… │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Segment_Input_Layer           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ Context_Vector_Input_Layer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ modified_standard_decoder_1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31000\u001b[0m)       │      \u001b[38;5;34m87,010,328\u001b[0m │ Segment_Input_Layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mModifiedStandardDecoder\u001b[0m)     │                           │                 │ Context_Vector_Input_Laye… │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,010,328</span> (331.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m87,010,328\u001b[0m (331.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,010,328</span> (331.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m87,010,328\u001b[0m (331.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Configuration\n",
    "vocab_size = 31000\n",
    "embedding_dim = 768\n",
    "num_heads = 8\n",
    "ff_dim = 1024\n",
    "dropout_rate = 0.1\n",
    "\n",
    "decoder = ModifiedStandardDecoder(vocab_size, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Inputs to the model\n",
    "segment_inputs = tf.keras.Input(shape=(None,), name=\"Segment_Input_Layer\") # refers to the tokens fed into one at a time\n",
    "encoder_outputs = tf.keras.Input(shape=(None, embedding_dim), name=\"Context_Vector_Input_Layer\")\n",
    "\n",
    "padding_mask_layer = PaddingMaskLayer(num_heads=num_heads, name=\"Padding_Mask_Layer\")\n",
    "padding_mask = padding_mask_layer(segment_inputs)\n",
    "\n",
    "outputs = decoder(segment_inputs, encoder_outputs, padding_mask=padding_mask)\n",
    "model_issues = tf.keras.Model(inputs=[encoder_outputs, segment_inputs], outputs=outputs)\n",
    "\n",
    "model_issues.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc345392-078f-4c78-9102-661b2ebaf8af",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ccb7311-488d-4454-bc70-a80535e9e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of initial dataframe:  (325, 4)\n",
      "shape of dataframe when null values were dropped:  (325, 4)\n",
      "shape of dataframe when preprocessed and duplicated values where dropped:  (325, 4)\n"
     ]
    }
   ],
   "source": [
    "x = preprocess(\"new_court_cases.csv\")\n",
    "court_cases = x.give_courtcases()\n",
    "facts = x.give_rfi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd8d10-bcc7-454b-bdd6-ebaa75dcb89e",
   "metadata": {},
   "source": [
    "# Instantiate Legal BERT Preprocessor and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4160e5-274d-4d2d-855b-4773d98cbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdfl0\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the preprocessor and legal BERT\n",
    "legal_bert = LegalBert()\n",
    "\n",
    "# Get context vectors of the whole court cases in batches\n",
    "bert_output = legal_bert.get_context_vectors(court_cases, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbbb17f-995b-4083-ae0b-0c2172817e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([325, 1024, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db9472-76d0-4537-9b77-31d7332dae3b",
   "metadata": {},
   "source": [
    "# Tokenize and add paddings to each data for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2c39e9-04c7-4de0-a6cd-8a5608cccb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_issues = legal_bert.tokenizer\n",
    "# Tokenize the issues segments using LegalBERT tokenizer\n",
    "tokenized_segments_issues = tokenizer_issues(issues, padding=True, truncation=True, max_length=1024, return_tensors='tf')\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids = tokenized_segments_issues['input_ids']\n",
    "\n",
    "# Shift the input sequences to the right by one position\n",
    "shifted_segments_issues = np.zeros_like(input_ids.numpy())\n",
    "shifted_segments_issues[:, 1:] = input_ids[:, :-1]  # Shift right\n",
    "shifted_segments_issues[:, 0] = tokenizer_issues.cls_token_id  # Use BERT's [CLS] token ID as the start token\n",
    "\n",
    "# Convert shifted sequences to tensor\n",
    "shifted_segments_issues = tf.convert_to_tensor(shifted_segments_issues)\n",
    "\n",
    "# Set the court cases to xtrain\n",
    "xbert_train = bert_output.detach().numpy()\n",
    "xtrain = tf.convert_to_tensor(xbert_train, dtype=tf.float32)\n",
    "\n",
    "# Set the segments to the ytrain and ytrain shifted\n",
    "ytrain_issues = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "ytrain_shifted = tf.convert_to_tensor(shifted_segments_issues, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf214c3-feee-4f7b-ab2b-001af2bcfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape: (325, 1024, 768)\n",
      "ytrain_shifted shape: (325, 1024)\n",
      "ytrain_issues shape: (325, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtrain shape:\", xtrain.shape)  # Should be (batch_size, seq_len, embedding_dim)\n",
    "print(\"ytrain_shifted shape:\", ytrain_shifted.shape)  # Should be (batch_size, seq_len)\n",
    "print(\"ytrain_issues shape:\", ytrain_issues.shape)  # Should be (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25e303-92ee-4ce1-af65-06e223bb0bd6",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0389dd2-8151-4843-b4f7-ab6565dd4889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m822s\u001b[0m 72s/step - accuracy: 0.0412 - loss: 12.6687\n",
      "Epoch 2/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m778s\u001b[0m 70s/step - accuracy: 0.0623 - loss: 9.0652\n",
      "Epoch 3/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m888s\u001b[0m 77s/step - accuracy: 0.0403 - loss: 9.3904\n"
     ]
    }
   ],
   "source": [
    "# Compile the models\n",
    "model_issues.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_issues.fit([xtrain, ytrain_issues], ytrain_shifted, epochs=3)\n",
    "model_issues.save('facts_seq_to_seq.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faee3c36-6dde-4c3a-8ee9-9412d5aa84e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"functional_5_1/Cast:0\", shape=(None, 1024), dtype=float32). Expected shape (None, None, 768), but input has incompatible shape (None, 1024)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None, 1024), dtype=int32)', 'tf.Tensor(shape=(None, 1024, 768), dtype=float32)')\n  • training=True\n  • mask=('None', 'None')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample Prediction after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Apply the callback during training\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel_issues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mytrain_issues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain_issues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mPrintPredictionsCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py:280\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"functional_5_1/Cast:0\", shape=(None, 1024), dtype=float32). Expected shape (None, None, 768), but input has incompatible shape (None, 1024)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None, 1024), dtype=int32)', 'tf.Tensor(shape=(None, 1024, 768), dtype=float32)')\n  • training=True\n  • mask=('None', 'None')"
     ]
    }
   ],
   "source": [
    "class PrintPredictionsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Prepare a sample input (tokenized and padded sequence)\n",
    "        sample_court_case_input = ytrain_issues[:1]  # Shape: (1, sequence_length)\n",
    "        sample_context_vector_input = xtrain[:1]  # Shape: (1, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Make a prediction\n",
    "        sample_output = self.model.predict([sample_court_case_input, sample_context_vector_input])\n",
    "        \n",
    "        # Decode the prediction\n",
    "        decoded_output = tokenizer_issues.sequences_to_texts(sample_output.argmax(-1))\n",
    "        \n",
    "        print(f\"\\nSample Prediction after epoch {epoch+1}: {decoded_output}\")\n",
    "\n",
    "# Apply the callback during training\n",
    "model_issues.fit([ytrain_issues, xtrain], ytrain_issues, epochs=3, callbacks=[PrintPredictionsCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed39f50-9c1b-47d5-8964-200819305eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
